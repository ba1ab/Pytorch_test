{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab6bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "LAMBDA_E = 0.5\n",
    "LAMBDA_T = 0.5\n",
    "MIN_SIZE = 1 # MB  1024 * 8 \n",
    "MAX_SIZE = 50 # MB 1024 * 8\n",
    "MIN_CYCLE = 300 # \n",
    "MAX_CYCLE = 1000\n",
    "MIN_DDL = 0.1 # seconds\n",
    "MAX_DDL = 1 # seconds\n",
    "MIN_RESOURCE = 0.4 # GHz\n",
    "MAX_RESOURCE = 1.5 # GHz\n",
    "MIN_POWER = 1 # dB\n",
    "MAX_POWER = 24\n",
    "CAPABILITY_E = 4 # GHz \n",
    "K_ENERGY_LOCAL = 5 * 1e-27\n",
    "\n",
    "MIN_ENE =0.5\n",
    "MAX_ENE = 3.2\n",
    "HARVEST_RATE = 0.001\n",
    "W_BANDWIDTH = 40\n",
    "\n",
    "K_CHANNEL = 10\n",
    "S_E = 400\n",
    "N_UNITS = 8\n",
    "MAX_STEPS = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f7310b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class uavENV(object):\n",
    "\n",
    "    \n",
    "\n",
    "    def __init__(self, n_agents):\n",
    "        \n",
    "        self.state_size = 7\n",
    "        self.action_size = 3\n",
    "        self.n_agents = n_agents\n",
    "        self.W_BANDWIDTH = W_BANDWIDTH\n",
    "\n",
    "        self.S_power = np.zeros(self.n_agents)\n",
    "        self.initial_energy = np.zeros(self.n_agents)\n",
    "        self.S_energy = np.zeros(self.n_agents)\n",
    "        self.S_gain = np.zeros(self.n_agents)\n",
    "        self.S_size = np.zeros(self.n_agents)\n",
    "        self.S_cycle = np.zeros(self.n_agents)  \n",
    "        self.S_ddl = np.zeros(self.n_agents)\n",
    "        self.S_res = np.zeros(self.n_agents)\n",
    "        self.action_lower_bound = [0,  0.01, 0.01] \n",
    "        self.action_higher_bound = [1, 1, 1]\n",
    "        for n in range(self.n_agents):\n",
    "            self.S_size[n] = np.random.uniform(MIN_SIZE, MAX_SIZE)\n",
    "            self.S_cycle[n] = np.random.uniform(MIN_CYCLE, MAX_CYCLE)\n",
    "            self.S_ddl[n] = np.random.uniform(MIN_DDL, MAX_DDL)\n",
    "            self.S_res[n] = np.random.uniform(MIN_RESOURCE, MAX_RESOURCE)\n",
    "\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "        for n in range(self.n_agents):\n",
    "            self.S_size[n] = np.random.uniform(MIN_SIZE, MAX_SIZE)\n",
    "            self.S_cycle[n] = np.random.uniform(MIN_CYCLE, MAX_CYCLE)\n",
    "            self.S_ddl[n] = np.random.uniform(MIN_DDL, MAX_DDL)\n",
    "            self.S_energy[n] = deepcopy(self.initial_energy[n])\n",
    "        self.S_energy = np.clip(self.S_energy, MIN_ENE, MAX_ENE)\n",
    "        self.state = np.array([[self.S_power[n], self.S_gain[n], self.S_energy[n], self.S_size[n], self.S_cycle[n], self.S_ddl[n], self.S_res[n]] for n in range(self.n_agents)])\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step_mec(self, action):\n",
    "        A_decision = np.zeros(self.n_agents)\n",
    "        A_res = np.zeros(self.n_agents)\n",
    "        A_power = np.zeros(self.n_agents)\n",
    "        for n in range(self.n_agents):\n",
    "            A_decision[n] = action[n][0]\n",
    "            A_res[n] = action[n][1] * self.S_res[n] * 10 ** 9\n",
    "            A_power[n] = action[n][2] * 10 ** ((self.S_power[n]-30)/10)\n",
    "\n",
    "        # 任务时间计算\n",
    "        x_n = A_decision\n",
    "        DataRate = self.W_BANDWIDTH * 10 ** 6  * np.log(1 + A_power * 10 **(self.S_gain/10)) / np.log(2)\n",
    "        DataRate = DataRate / K_CHANNEL\n",
    "        Time_proc = self.S_size*8*1024*self.S_cycle / (CAPABILITY_E*10**9)\n",
    "        Time_local = self.S_size*8*1024*self.S_cycle / A_res\n",
    "        Time_max_local = self.S_size*8*1024*self.S_cycle / (MIN_RESOURCE*10**9)\n",
    "        Time_off = self.S_size*8*1024/ DataRate\n",
    "        for i in range(self.n_agents):\n",
    "            if x_n[i] == 2:\n",
    "                Time_off[i] = MAX_DDL\n",
    "                x_n[i] = 1\n",
    "        Time_finish = np.zeros(self.n_agents)\n",
    "        SortedOFF = np.argsort(Time_off)\n",
    "        MECtime = np.zeros(N_UNITS)\n",
    "        counting = 0\n",
    "        for i in range(self.n_agents):\n",
    "            if x_n[SortedOFF[i]] == 1 and counting < N_UNITS:\n",
    "                Time_finish[SortedOFF[i]] = Time_local[SortedOFF[i]] + Time_proc[SortedOFF[i]]\n",
    "                MECtime[np.argmin(MECtime)] = Time_local[SortedOFF[i]] + Time_proc[SortedOFF[i]]\n",
    "                counting += 1\n",
    "            elif x_n[SortedOFF[i]] == 1:\n",
    "                for j in range(i):\n",
    "                    if x_n[SortedOFF[j]] == 1:\n",
    "                        MECtime[np.argmin(MECtime)] += Time_proc[SortedOFF[j]]\n",
    "                Time_finish[SortedOFF[i]] = max(Time_off[SortedOFF[i]], np.min(MECtime)) + Time_proc[SortedOFF[i]]\n",
    "                MECtime[np.argmin(MECtime)] = max(Time_off[SortedOFF[i]], np.min(MECtime)) + Time_proc[SortedOFF[i]]\n",
    "        Time_n = (1-x_n) * Time_local + x_n * (Time_off + Time_proc)\n",
    "\n",
    "        Time_n = [min(t,MAX_DDL) / MAX_DDL for t in Time_n]\n",
    "        T_mean = np.mean(Time_n)\n",
    "\n",
    "        # 能耗计算\n",
    "        Energy_local = K_ENERGY_LOCAL * self.S_size *8*1024 * self.S_cycle * A_res\n",
    "        Energy_max_local = K_ENERGY_LOCAL * self.S_size *8*1024* self.S_cycle * (self.S_res*10**9)\n",
    "        Energy_off = A_power * Time_off\n",
    "        Energy_n = (1-x_n) * Energy_local + x_n * Energy_off\n",
    "        self.S_energy = np.clip(self.S_energy - Energy_n*1e-6 + np.random.normal(HARVEST_RATE, 0, size=self.n_agents)*1e-6, 0, MAX_ENE)\n",
    "        for i in range(x_n.size):\n",
    "            if self.S_energy[i] <= 0:\n",
    "                Time_n[i] = MAX_DDL / MIN_DDL\n",
    "        \n",
    "        # 奖励计算\n",
    "        \n",
    "        Time_penalty = np.maxmum((Time_n - self.S_ddl/MAX_DDL), 0)\n",
    "        Energy_penalty = np.maximum((MIN_ENE - self.S_energy), 0)*10**6\n",
    "        time_penalty_nozero_count = np.count_nonzero(Time_penalty)/self.n_agents\n",
    "        energy_penalty_nozero_count = np.count_nonzero(Energy_penalty)/self.n_agents\n",
    "        Reward = -1*(LAMBDA_E * np.array(Energy_n) + LAMBDA_T * np.arrat(Time_n)) -1*(LAMBDA_E *np.array(Energy_penalty) + LAMBDA_T*np.array(Time_penalty))\n",
    "        Reward = np.ones_lik(Reward) * np.sum(Reward)\n",
    "        for n in range(self.n_agents):\n",
    "            self.S_size[n] = np.random.uniform(MIN_SIZE, MAX_SIZE)\n",
    "            self.S_cycle[n] = np.random.uniform(MIN_CYCLE, MAX_CYCLE)\n",
    "            self.S_ddl[n] = np.random.uniform(MIN_DDL, MAX_DDL - MIN_DDL/10)\n",
    "        \n",
    "        # 状态更新\n",
    "        self.state = np.array([[self.S_power[n], self.S_gain[n], self.S_energy[n], self.S_size[n], self.S_cycle[n], self.S_ddl[n], self.S_res[n]] for n in range(self.n_agents)])\n",
    "        self.step += 1\n",
    "        done = False\n",
    "        if self.step >= MAX_STEPS:\n",
    "            self.step = 0\n",
    "            done = True\n",
    "        return self.state, Reward, done, energy_penalty_nozero_count, time_penalty_nozero_count\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f155af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[0.00000000e+00 0.00000000e+00 5.00000000e-01 3.67507027e+01\n",
      "  7.46290230e+02 8.98491468e-01 1.08562794e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.00000000e-01 2.41385313e+01\n",
      "  3.83715972e+02 7.41920309e-01 7.57701654e-01]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAll tests passed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m result = \u001b[43mtest_mec_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTest Result:\u001b[39m\u001b[33m\"\u001b[39m, result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtest_mec_env\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m action = np.array([\u001b[32m0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m])  \u001b[38;5;66;03m# Agent 1 local, Agent 2 offload\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Test step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m next_state, reward, done, energy_penalty_count, time_penalty_count = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_mec\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNext State:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(next_state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36muavENV.step_mec\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     46\u001b[39m A_power = np.zeros(\u001b[38;5;28mself\u001b[39m.n_agents)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_agents):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     A_decision[n] = \u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     49\u001b[39m     A_res[n] = action[n][\u001b[32m1\u001b[39m] * \u001b[38;5;28mself\u001b[39m.S_res[n] * \u001b[32m10\u001b[39m ** \u001b[32m9\u001b[39m\n\u001b[32m     50\u001b[39m     A_power[n] = action[n][\u001b[32m2\u001b[39m] * \u001b[32m10\u001b[39m ** ((\u001b[38;5;28mself\u001b[39m.S_power[n]-\u001b[32m30\u001b[39m)/\u001b[32m10\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "def test_mec_env():\n",
    "    \n",
    "    n_agents = 2\n",
    "    env = uavENV(n_agents)\n",
    "\n",
    "    # Test reset\n",
    "    state = env.reset()\n",
    "    print(\"Initial State:\")\n",
    "    print(state)\n",
    "    if state.shape != (n_agents, 7):\n",
    "        return \"Error: State shape incorrect\"\n",
    "    if not np.all((state[:, 2] >= MIN_ENE) & (state[:, 2] <= MAX_ENE)):\n",
    "        return \"Error: Energy out of bounds\"\n",
    "\n",
    "    # Generate sample actions (normalized between 0 and 1)\n",
    "    # Action format: [decision (0 or 1), res_fraction, power_fraction]\n",
    "    action = np.array([[0, 0.5, 0.5], [1, 0.7, 0.8]])  # Agent 1 local, Agent 2 offload\n",
    "\n",
    "    # Test step\n",
    "    next_state, reward, done, energy_penalty_count, time_penalty_count = env.step_mec(action)\n",
    "    print(\"\\nNext State:\")\n",
    "    print(next_state)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Energy Penalty Count:\", energy_penalty_count)\n",
    "    print(\"Time Penalty Count:\", time_penalty_count)\n",
    "\n",
    "    # Basic checks\n",
    "    if done:\n",
    "        return \"Error: Done should be False after one step\"\n",
    "    if not np.all(reward < 0):  # Rewards are negative as per formula\n",
    "        return \"Error: Reward should be negative\"\n",
    "    if energy_penalty_count < 0 or energy_penalty_count > 1:\n",
    "        return \"Error: Energy penalty count out of range\"\n",
    "    if time_penalty_count < 0 or time_penalty_count > 1:\n",
    "        return \"Error: Time penalty count out of range\"\n",
    "\n",
    "    # Check energy update (should decrease or stay within bounds)\n",
    "    if not np.all((next_state[:, 2] >= 0) & (next_state[:, 2] <= MAX_ENE)):\n",
    "        return \"Error: Next energy out of bounds\"\n",
    "\n",
    "    # Run multiple steps to reach MAX_STEPS\n",
    "    for _ in range(MAX_STEPS - 1):\n",
    "        next_state, reward, done, _, _ = env.step_mec(action)\n",
    "    if not done:\n",
    "        return \"Error: Done should be True after MAX_STEPS\"\n",
    "\n",
    "    return \"All tests passed\"\n",
    "\n",
    "# Run the test\n",
    "result = test_mec_env()\n",
    "print(\"\\nTest Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
